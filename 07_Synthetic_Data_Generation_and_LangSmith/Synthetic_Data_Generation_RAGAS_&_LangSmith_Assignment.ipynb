{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCk2Rx4cjlYF"
      },
      "source": [
        "# Synthetic Data Generation Using RAGAS - RAG Evaluation with LangSmith\n",
        "\n",
        "In the following notebook we'll explore a use-case for RAGAS' synthetic testset generation workflow!\n",
        "\n",
        "\n",
        "\n",
        "- ğŸ¤ BREAKOUT ROOM #1\n",
        "  1. Use RAGAS to Generate Synthetic Data\n",
        "\n",
        "- ğŸ¤ BREAKOUT ROOM #2\n",
        "  1. Load them into a LangSmith Dataset\n",
        "  2. Evaluate our RAG chain against the synthetic test data\n",
        "  3. Make changes to our pipeline\n",
        "  4. Evaluate the modified pipeline\n",
        "\n",
        "SDG is a critical piece of the puzzle, especially for early iteration! Without it, it would not be nearly as easy to get high quality early signal for our application's performance.\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bG2ta-B478G"
      },
      "source": [
        "# ğŸ¤ BREAKOUT ROOM #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VUI7vF_kbv9"
      },
      "source": [
        "## Task 1: Dependencies and API Keys\n",
        "\n",
        "We'll need to install a number of API keys and dependencies, since we'll be leveraging a number of great technologies for this pipeline!\n",
        "\n",
        "1. OpenAI's endpoints to handle the Synthetic Data Generation\n",
        "2. OpenAI's Endpoints for our RAG pipeline and LangSmith evaluation\n",
        "3. QDrant as our vectorstore\n",
        "4. LangSmith for our evaluation coordinator!\n",
        "\n",
        "Let's install and provide all the required information below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies and API Keys:\n",
        "\n",
        "> NOTE: DO NOT RUN THESE CELLS IF YOU ARE RUNNING THIS NOTEBOOK LOCALLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/175.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m175.7/175.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/71.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m160.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#!pip install -qU ragas==0.2.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NLTK Import\n",
        "\n",
        "To prevent errors that may occur based on OS - we'll import NLTK and download the needed packages to ensure correct handling of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "[nltk_data] Downloading package punkt to /home/jupiter-\n",
            "[nltk_data]     core/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/jupiter-core/nltk_data...\n",
=======
            "[nltk_data] Downloading package punkt to /Users/aqsa/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/aqsa/nltk_data...\n",
>>>>>>> d2a6684 (Updated Assignment 7)
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also want to set a project name to make things easier for ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - SDG - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenAI's API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Synthetic Test Data\n",
        "\n",
        "We wil be using Ragas to build out a set of synthetic test questions, references, and reference contexts. This is useful because it will allow us to find out how our system is performing.\n",
        "\n",
        "> NOTE: Ragas is best suited for finding *directional* changes in your LLM-based systems. The absolute scores aren't comparable in a vacuum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation\n",
        "\n",
        "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
        "\n",
        "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
        "\n",
        "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
        "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
        "\n",
        "Let's start by collecting our data into a useful pile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
<<<<<<< HEAD
            "100 31287    0 31287    0     0  92995      0 --:--:-- --:--:-- --:--:-- 92839\n"
=======
            "100 31524    0 31524    0     0  54129      0 --:--:-- --:--:-- --:--:-- 54072\n"
>>>>>>> d2a6684 (Updated Assignment 7)
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
<<<<<<< HEAD
            "100 70146    0 70146    0     0   182k      0 --:--:-- --:--:-- --:--:--  183k\n"
=======
            "100 70549    0 70549    0     0   592k      0 --:--:-- --:--:-- --:--:--  593k\n"
>>>>>>> d2a6684 (Updated Assignment 7)
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's load our data into a familiar LangChain format using the `DirectoryLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.html\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knowledge Graph Based Synthetic Generation\n",
        "\n",
        "Ragas uses a knowledge graph based approach to create data. This is extremely useful as it allows us to create complex queries rather simply. The additional testset complexity allows us to evaluate larger problems more effectively, as systems tend to be very strong on simple evaluation tasks.\n",
        "\n",
        "Let's start by defining our `generator_llm` (which will generate our questions, summaries, and more), and our `generator_embeddings` which will be useful in building our graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unrolled SDG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
<<<<<<< HEAD
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupiter-core/Code/AIMS/Courses/AIE6 Staging/07_Synthetic_Data_Generation_and_LangSmith/.venv/lib/python3.13/site-packages/pysbd/segmenter.py:66: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  for match in re.finditer('{0}\\s*'.format(re.escape(sent)), self.original_text):\n",
            "/home/jupiter-core/Code/AIMS/Courses/AIE6 Staging/07_Synthetic_Data_Generation_and_LangSmith/.venv/lib/python3.13/site-packages/pysbd/lang/arabic.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  txt = re.sub('(?<={0})\\.'.format(am), 'âˆ¯', txt)\n",
            "/home/jupiter-core/Code/AIMS/Courses/AIE6 Staging/07_Synthetic_Data_Generation_and_LangSmith/.venv/lib/python3.13/site-packages/pysbd/lang/persian.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  txt = re.sub('(?<={0})\\.'.format(am), 'âˆ¯', txt)\n"
          ]
        }
      ],
=======
      "outputs": [],
>>>>>>> d2a6684 (Updated Assignment 7)
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we're going to instantiate our Knowledge Graph.\n",
        "\n",
        "This graph will contain N number of nodes that have M number of relationships. These nodes and relationships (AKA \"edges\") will define our knowledge graph and be used later to construct relevant questions and responses."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 6,
=======
      "execution_count": 12,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 0, relationships: 0)"
            ]
          },
<<<<<<< HEAD
          "execution_count": 6,
=======
          "execution_count": 12,
>>>>>>> d2a6684 (Updated Assignment 7)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.graph import KnowledgeGraph\n",
        "\n",
        "kg = KnowledgeGraph()\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step we're going to take is to simply insert each of our full documents into the graph. This will provide a base that we can apply transformations to."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 7,
=======
      "execution_count": 13,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 2, relationships: 0)"
            ]
          },
<<<<<<< HEAD
          "execution_count": 7,
=======
          "execution_count": 13,
>>>>>>> d2a6684 (Updated Assignment 7)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.graph import Node, NodeType\n",
        "\n",
        "for doc in docs:\n",
        "    kg.nodes.append(\n",
        "        Node(\n",
        "            type=NodeType.DOCUMENT,\n",
        "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
        "        )\n",
        "    )\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we'll apply the *default* transformations to our knowledge graph. This will take the nodes currently on the graph and transform them based on a set of [default transformations](https://docs.ragas.io/en/latest/references/transforms/#ragas.testset.transforms.default_transforms).\n",
        "\n",
        "These default transformations are dependent on the corpus length, in our case:\n",
        "\n",
        "- Producing Summaries -> produces summaries of the documents\n",
        "- Extracting Headlines -> finding the overall headline for the document\n",
        "- Theme Extractor -> extracts broad themes about the documents\n",
        "\n",
        "It then uses cosine-similarity and heuristics between the embeddings of the above transformations to construct relationships between the nodes."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 8,
=======
      "execution_count": 14,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "af32d72b6b764be6a9293b0d808debfc",
=======
              "model_id": "4750d2aaa0a44dd280a825f2e883b5a0",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "e5f18f26a6c3497fbaa1310487358356",
=======
              "model_id": "40d36a526e604aea86f409d9e52ec7d6",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "a166b9dd7bd34f6191edf00697f325b5",
=======
              "model_id": "6898cef5dd5b4ce8844709a5b93a0c45",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "0318c1f689a4427f8af3403ea013b232",
=======
              "model_id": "d68096061aa048f9bcf70230e24488fc",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "e02ddbb7444b40f09894983f313c55d2",
=======
              "model_id": "1c100081cf58497e8eabdaf23c109795",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
<<<<<<< HEAD
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/18 [00:00<?, ?it/s]"
=======
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/24 [00:00<?, ?it/s]"
>>>>>>> d2a6684 (Updated Assignment 7)
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "unable to apply transformation: 'StringIO' object has no attribute 'output'\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "d5e346afa20940fd87f8bbb02ec9681a",
=======
              "model_id": "29c3bd0486574d5f8643eb0e80597456",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "KnowledgeGraph(nodes: 10, relationships: 26)"
            ]
          },
          "execution_count": 8,
=======
              "KnowledgeGraph(nodes: 13, relationships: 46)"
            ]
          },
          "execution_count": 14,
>>>>>>> d2a6684 (Updated Assignment 7)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "\n",
        "transformer_llm = generator_llm\n",
        "embedding_model = generator_embeddings\n",
        "\n",
        "default_transforms = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
        "apply_transforms(kg, default_transforms)\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can save and load our knowledge graphs as follows."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 9,
=======
      "execution_count": 15,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "KnowledgeGraph(nodes: 10, relationships: 26)"
            ]
          },
          "execution_count": 9,
=======
              "KnowledgeGraph(nodes: 13, relationships: 46)"
            ]
          },
          "execution_count": 15,
>>>>>>> d2a6684 (Updated Assignment 7)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kg.save(\"ai_across_years_kg.json\")\n",
        "ai_across_years_kg = KnowledgeGraph.load(\"ai_across_years_kg.json\")\n",
        "ai_across_years_kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using our knowledge graph, we can construct a \"test set generator\" - which will allow us to create queries."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 10,
=======
      "execution_count": 16,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=ai_across_years_kg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, we'd like to be able to define the kinds of queries we're generating - which is made simple by Ragas having pre-created a number of different \"QuerySynthesizer\"s.\n",
        "\n",
        "Each of these Synthetsizers is going to tackle a separate kind of query which will be generated from a scenario and a persona.\n",
        "\n",
        "In essence, Ragas will use an LLM to generate a persona of someone who would interact with the data - and then use a scenario to construct a question from that data and persona."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 11,
=======
      "execution_count": 17,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
        "\n",
        "query_distribution = [\n",
        "        (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
        "        (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "        (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "What are the three types of query synthesizers doing? Describe each one in simple terms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
<<<<<<< HEAD
=======
        "#### â“ Answer #1:\n",
        "\n",
        "1) Single Hop Specific Query: Goes to only one place to answer the query (Para says Albert Was born IN Jamaica. If the question is Where was Albert born? It answers using the single hop):  Focused on one KG edge. A \n",
        "A typical usecase is Baseline factual QA; helpful for grounding the system on entity-property-value relations.\n",
        "\n",
        "2) Multi Hop Abstract Query: Multiple sources(i.e Involves multi-edge reasoning, it needs to traverse to multiple edges to answer the question). Also, it will answer a abstract question, but not a simple conceptual question(not a fact). ie answer harder/open ended questions. It does so by connecting multiple facts that may not be tied to specific entities. Usecase include  stress-test multi-hop reasoning without giving away paths explicitly.\n",
        "\n",
        "3) Multi Hop Specific Query: Multiple sources/chunks, and answers a factual question. Ie. It generates multi-hop questions that are detailed and grounded in specific entities. Usecase includes Evaluating systems' ability to track and chain factual knowledg\n",
        "\n",
        "The second parameter is assigning weights distribution to the synthesize ( 50% simple factual (Single-Hop), 25% thematic, hard general reasoning (Multi-Hop Abstract), 25% hard, but specific reasoning (Multi-Hop Specific)) \n",
        "This ensures Breadth: Variety in question types; Depth: Evaluates system's single-hop vs multi-hop reasoning; Realism: Mimics real-world query distribution (some simple, some vague, some difficult)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
>>>>>>> d2a6684 (Updated Assignment 7)
        "Finally, we can use our `TestSetGenerator` to generate our testset!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 12,
=======
      "execution_count": 18,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "01a48fa109d045a29261c4123337a1c0",
=======
              "model_id": "ff36e034e3d24028b8f6462a4b60d079",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "5f218b66f1674b53806c95eb3b8cfa0d",
=======
              "model_id": "959e2cb503bc43d1b6ca2b0c1c2518c6",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "5bc70774a07244dcb38ecdccdba3aa79",
=======
              "model_id": "011eb315ac0b4bcb929fb726f8bf1f80",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/11 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
<<<<<<< HEAD
              "      <td>Is Baidu involved in developing large language...</td>\n",
              "      <td>[The ethics of this space remain diabolically ...</td>\n",
              "      <td>The context mentions that several organization...</td>\n",
=======
              "      <td>What role does OpenAI play in the development ...</td>\n",
              "      <td>[The ethics of this space remain diabolically ...</td>\n",
              "      <td>According to the provided context, OpenAI was ...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
<<<<<<< HEAD
              "      <td>Who is Simon Willison in the context of AI dev...</td>\n",
              "      <td>[Simon Willisonâ€™s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>Simon Willison is mentioned in the context of ...</td>\n",
=======
              "      <td>What is the role of Artifical Intelligance in AI?</td>\n",
              "      <td>[Simon Willisonâ€™s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>Artificial Intelligence is the academic field ...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
<<<<<<< HEAD
              "      <td>What is RedMonk?</td>\n",
              "      <td>[the document includes some of the clearest ex...</td>\n",
              "      <td>The context does not provide a specific explan...</td>\n",
=======
              "      <td>How are Large Language Models (LLMs) impacting...</td>\n",
              "      <td>[the document includes some of the clearest ex...</td>\n",
              "      <td>The context discusses the profound impact of L...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
<<<<<<< HEAD
              "      <td>Wha is Qwen2.5-Coder?</td>\n",
              "      <td>[The rise of inference-scaling â€œreasoningâ€ mod...</td>\n",
              "      <td>Qwen2.5-Coder is an LLM that can code well and...</td>\n",
=======
              "      <td>What is Mistral in the context of AI models?</td>\n",
              "      <td>[The environmental impact got much, much worse...</td>\n",
              "      <td>Mistral is listed among the organizations with...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
<<<<<<< HEAD
              "      <td>How does ChatGPT's voice mode compare to Googl...</td>\n",
              "      <td>[OpenAI arenâ€™t the only group with a multi-mod...</td>\n",
              "      <td>ChatGPT voice mode now provides the option to ...</td>\n",
=======
              "      <td>What is Qwen2-VL and how does it relate to mul...</td>\n",
              "      <td>[and generic, but my phone can pitch bland and...</td>\n",
              "      <td>Qwen2-VL is mentioned as part of the recent tr...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
<<<<<<< HEAD
              "      <td>How development of LLMs like GPT-4 is like, an...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>Simon Willisonâ€™s weblog states that 2023 was a...</td>\n",
=======
              "      <td>how GPT-4 breakthroughs compare to current AI ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThe ethics of this space remain di...</td>\n",
              "      <td>The context shows that despite the limitations...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
<<<<<<< HEAD
              "      <td>how voice and live camera integration and voic...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nOpenAI arenâ€™t the only group with ...</td>\n",
              "      <td>The context shows that multimodal models like ...</td>\n",
=======
              "      <td>how much cost training llms and is it worth it...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThe ethics of this space remain di...</td>\n",
              "      <td>The context explains that training large langu...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
<<<<<<< HEAD
              "      <td>What are the recent advancements in Large Lang...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>Recent advancements in Large Language Models (...</td>\n",
=======
              "      <td>how does the environmental impact of ai and lm...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThe environmental impact got much,...</td>\n",
              "      <td>the context shows that the environmental impac...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
<<<<<<< HEAD
              "      <td>What are the key developments and themes in la...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>In 2024, significant advancements in large lan...</td>\n",
=======
              "      <td>Considering the significant advancements and k...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>The developments in 2024 highlight a rapidly e...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
<<<<<<< HEAD
              "      <td>Hwo meta is involved in inference scalin and w...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nchat interface on November 20th. T...</td>\n",
              "      <td>Meta published a relevant paper titled 'Traini...</td>\n",
=======
              "      <td>How do the recent advancements in voice and vi...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nthe document includes some of the ...</td>\n",
              "      <td>The context highlights that models like Google...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
<<<<<<< HEAD
              "      <td>how ChatGPT and other models like Gemini and A...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nthe document includes some of the ...</td>\n",
              "      <td>The context explains that ChatGPT's voice mode...</td>\n",
=======
              "      <td>How does Google's involvement in LLMs relate t...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nthe document includes some of the ...</td>\n",
              "      <td>The context highlights that Google has develop...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
<<<<<<< HEAD
              "0   Is Baidu involved in developing large language...   \n",
              "1   Who is Simon Willison in the context of AI dev...   \n",
              "2                                    What is RedMonk?   \n",
              "3                               Wha is Qwen2.5-Coder?   \n",
              "4   How does ChatGPT's voice mode compare to Googl...   \n",
              "5   How development of LLMs like GPT-4 is like, an...   \n",
              "6   how voice and live camera integration and voic...   \n",
              "7   What are the recent advancements in Large Lang...   \n",
              "8   What are the key developments and themes in la...   \n",
              "9   Hwo meta is involved in inference scalin and w...   \n",
              "10  how ChatGPT and other models like Gemini and A...   \n",
=======
              "0   What role does OpenAI play in the development ...   \n",
              "1   What is the role of Artifical Intelligance in AI?   \n",
              "2   How are Large Language Models (LLMs) impacting...   \n",
              "3        What is Mistral in the context of AI models?   \n",
              "4   What is Qwen2-VL and how does it relate to mul...   \n",
              "5   how GPT-4 breakthroughs compare to current AI ...   \n",
              "6   how much cost training llms and is it worth it...   \n",
              "7   how does the environmental impact of ai and lm...   \n",
              "8   Considering the significant advancements and k...   \n",
              "9   How do the recent advancements in voice and vi...   \n",
              "10  How does Google's involvement in LLMs relate t...   \n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [The ethics of this space remain diabolically ...   \n",
              "1   [Simon Willisonâ€™s Weblog Subscribe Stuff we fi...   \n",
              "2   [the document includes some of the clearest ex...   \n",
<<<<<<< HEAD
              "3   [The rise of inference-scaling â€œreasoningâ€ mod...   \n",
              "4   [OpenAI arenâ€™t the only group with a multi-mod...   \n",
              "5   [<1-hop>\\n\\nSimon Willisonâ€™s Weblog Subscribe ...   \n",
              "6   [<1-hop>\\n\\nOpenAI arenâ€™t the only group with ...   \n",
              "7   [<1-hop>\\n\\nSimon Willisonâ€™s Weblog Subscribe ...   \n",
              "8   [<1-hop>\\n\\nSimon Willisonâ€™s Weblog Subscribe ...   \n",
              "9   [<1-hop>\\n\\nchat interface on November 20th. T...   \n",
              "10  [<1-hop>\\n\\nthe document includes some of the ...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   The context mentions that several organization...   \n",
              "1   Simon Willison is mentioned in the context of ...   \n",
              "2   The context does not provide a specific explan...   \n",
              "3   Qwen2.5-Coder is an LLM that can code well and...   \n",
              "4   ChatGPT voice mode now provides the option to ...   \n",
              "5   Simon Willisonâ€™s weblog states that 2023 was a...   \n",
              "6   The context shows that multimodal models like ...   \n",
              "7   Recent advancements in Large Language Models (...   \n",
              "8   In 2024, significant advancements in large lan...   \n",
              "9   Meta published a relevant paper titled 'Traini...   \n",
              "10  The context explains that ChatGPT's voice mode...   \n",
=======
              "3   [The environmental impact got much, much worse...   \n",
              "4   [and generic, but my phone can pitch bland and...   \n",
              "5   [<1-hop>\\n\\nThe ethics of this space remain di...   \n",
              "6   [<1-hop>\\n\\nThe ethics of this space remain di...   \n",
              "7   [<1-hop>\\n\\nThe environmental impact got much,...   \n",
              "8   [<1-hop>\\n\\nSimon Willisonâ€™s Weblog Subscribe ...   \n",
              "9   [<1-hop>\\n\\nthe document includes some of the ...   \n",
              "10  [<1-hop>\\n\\nthe document includes some of the ...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   According to the provided context, OpenAI was ...   \n",
              "1   Artificial Intelligence is the academic field ...   \n",
              "2   The context discusses the profound impact of L...   \n",
              "3   Mistral is listed among the organizations with...   \n",
              "4   Qwen2-VL is mentioned as part of the recent tr...   \n",
              "5   The context shows that despite the limitations...   \n",
              "6   The context explains that training large langu...   \n",
              "7   the context shows that the environmental impac...   \n",
              "8   The developments in 2024 highlight a rapidly e...   \n",
              "9   The context highlights that models like Google...   \n",
              "10  The context highlights that Google has develop...   \n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   single_hop_specifc_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  "
            ]
          },
<<<<<<< HEAD
          "execution_count": 12,
=======
          "execution_count": 18,
>>>>>>> d2a6684 (Updated Assignment 7)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
        "testset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Abstracted SDG\n",
        "\n",
        "The above method is the full process - but we can shortcut that using the provided abstractions!\n",
        "\n",
        "This will generate our knowledge graph under the hood, and will - from there - generate our personas and scenarios to construct our queries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 13,
=======
      "execution_count": 19,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "b50ee065c9dc462291eeca3faaab8acd",
=======
              "model_id": "19f78f9aca4943e79d64ed2bee526bee",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "1a4e8868f02e416990325ba3fd1719bb",
=======
              "model_id": "b0356a4122d1463db82ad2a847403ed7",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "11169d7e12094a539918a0fea1113c15",
=======
              "model_id": "a8e883f34e1d427cb986fc5ac817131b",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "b6721e0bdf8444c7a7a474c4af2cea20",
=======
              "model_id": "20259399d1f14cb1acc212d2d5358bff",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "e7ae445fd17340719e7635989b9f5857",
=======
              "model_id": "e743cab1b8024f7aa1c5fdeacddf5951",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
<<<<<<< HEAD
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/22 [00:00<?, ?it/s]"
=======
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/18 [00:00<?, ?it/s]"
>>>>>>> d2a6684 (Updated Assignment 7)
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "292c7d1a9559416194242278d563c298",
=======
              "model_id": "a39822a48f8944cfb70f1dec1031e34e",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "2adc4ce127cc429cab5c1c910850b97a",
=======
              "model_id": "7f36b82aed73431098b5f024e1c3a152",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "381c80182be14840bc0f360f4b252376",
=======
              "model_id": "c69247cb9e34464bb23de1a8167d71c6",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "afe7997786384a2ba7ae6878bf04b0bb",
=======
              "model_id": "ff32134ef78045db8dd51be4a8e33893",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 22,
=======
      "execution_count": 20,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
<<<<<<< HEAD
              "      <td>Wut has Meta dun in the feeld of LLMs this yeer?</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>In February, Meta released Llama, and in July,...</td>\n",
=======
              "      <td>What is EleutherAI and how does it relate to t...</td>\n",
              "      <td>[The ethics of this space remain diabolically ...</td>\n",
              "      <td>According to the provided context, EleutherAI ...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
<<<<<<< HEAD
              "      <td>What significant event related to AI ethics oc...</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>In September last year, the term 'prompt injec...</td>\n",
=======
              "      <td>AI is what in 2023?</td>\n",
              "      <td>[Simon Willisonâ€™s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>In 2023, AI refers to Large Language Models (L...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
<<<<<<< HEAD
              "      <td>Whaat are the key highlights from Simon Willis...</td>\n",
              "      <td>[Simon Willisonâ€™s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>Simon Willison's Weblog highlights that 2023 w...</td>\n",
=======
              "      <td>What is OpenAI in context of large language mo...</td>\n",
              "      <td>[the document includes some of the clearest ex...</td>\n",
              "      <td>The context mentions OpenAI as one of the top ...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
<<<<<<< HEAD
              "      <td>What role does Stanford Alpaca play in the dev...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>Stanford Alpaca is associated with the acceler...</td>\n",
=======
              "      <td>Wha is OpenAI's role in the development of lar...</td>\n",
              "      <td>[Things we learned about LLMs in 2024 31st Dec...</td>\n",
              "      <td>The context does not provide specific informat...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
<<<<<<< HEAD
              "      <td>What are the ethical concerns associated with ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethical concerns associated with the gulli...</td>\n",
=======
              "      <td>How do AI model release strategies, like Qwen ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>The document discusses how effective AI model ...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
<<<<<<< HEAD
              "      <td>Why are Large Language Models (LLMs) considere...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>Large Language Models (LLMs) are considered bl...</td>\n",
=======
              "      <td>How does the dependence of agent capabilities ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nacknowledge that. If you tell me t...</td>\n",
              "      <td>The context highlights that the dependence of ...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
<<<<<<< HEAD
              "      <td>What are the ethical concerns related to the g...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethical concerns related to the gullibilit...</td>\n",
=======
              "      <td>How does the importance of training data quali...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nthis. LLMs need better criticism A...</td>\n",
              "      <td>The context emphasizes that the most critical ...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
<<<<<<< HEAD
              "      <td>How do the ethics of AI and the gullibility of...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nCode may be the best application T...</td>\n",
              "      <td>The ethics of AI and the gullibility of langua...</td>\n",
=======
              "      <td>How do the recent advancements in Large Langua...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>Recent advancements in Large Language Models (...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
<<<<<<< HEAD
              "      <td>How has the development of models surpassing G...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>In 2024, the development of models surpassing ...</td>\n",
=======
              "      <td>How does Google's recent advancements in large...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThings we learned about LLMs in 20...</td>\n",
              "      <td>In 2024, Google made significant strides in la...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
<<<<<<< HEAD
              "      <td>How does the training efficiency of DeepSeek v...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nday after that. DeepSeek v3 is a h...</td>\n",
              "      <td>DeepSeek v3, a 685B parameter model, is one of...</td>\n",
=======
              "      <td>Based on Simon Willisonâ€™s insights, how do the...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>Simon Willisonâ€™s weblog highlights that 2023 w...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
<<<<<<< HEAD
              "      <td>How did the breaking of the GPT-4 barrier in 2...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>In 2024, the breaking of the GPT-4 barrier sig...</td>\n",
=======
              "      <td>H0w do LLMs sh0w the progress in AI in 2023 an...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>The context indicates that 2023 was a breakthr...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
<<<<<<< HEAD
              "      <td>How has the introduction of GPT-4o impacted th...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>The introduction of GPT-4o has significantly i...</td>\n",
=======
              "      <td>Based on the insights about Appleâ€™s MLX librar...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willisonâ€™s Weblog Subscribe ...</td>\n",
              "      <td>The context highlights that Appleâ€™s MLX librar...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
<<<<<<< HEAD
              "0    Wut has Meta dun in the feeld of LLMs this yeer?   \n",
              "1   What significant event related to AI ethics oc...   \n",
              "2   Whaat are the key highlights from Simon Willis...   \n",
              "3   What role does Stanford Alpaca play in the dev...   \n",
              "4   What are the ethical concerns associated with ...   \n",
              "5   Why are Large Language Models (LLMs) considere...   \n",
              "6   What are the ethical concerns related to the g...   \n",
              "7   How do the ethics of AI and the gullibility of...   \n",
              "8   How has the development of models surpassing G...   \n",
              "9   How does the training efficiency of DeepSeek v...   \n",
              "10  How did the breaking of the GPT-4 barrier in 2...   \n",
              "11  How has the introduction of GPT-4o impacted th...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Simon Willisonâ€™s Weblog Subscribe Stuff we fi...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "5   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "6   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "7   [<1-hop>\\n\\nCode may be the best application T...   \n",
              "8   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "9   [<1-hop>\\n\\nday after that. DeepSeek v3 is a h...   \n",
=======
              "0   What is EleutherAI and how does it relate to t...   \n",
              "1                                 AI is what in 2023?   \n",
              "2   What is OpenAI in context of large language mo...   \n",
              "3   Wha is OpenAI's role in the development of lar...   \n",
              "4   How do AI model release strategies, like Qwen ...   \n",
              "5   How does the dependence of agent capabilities ...   \n",
              "6   How does the importance of training data quali...   \n",
              "7   How do the recent advancements in Large Langua...   \n",
              "8   How does Google's recent advancements in large...   \n",
              "9   Based on Simon Willisonâ€™s insights, how do the...   \n",
              "10  H0w do LLMs sh0w the progress in AI in 2023 an...   \n",
              "11  Based on the insights about Appleâ€™s MLX librar...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [The ethics of this space remain diabolically ...   \n",
              "1   [Simon Willisonâ€™s Weblog Subscribe Stuff we fi...   \n",
              "2   [the document includes some of the clearest ex...   \n",
              "3   [Things we learned about LLMs in 2024 31st Dec...   \n",
              "4   [<1-hop>\\n\\nSimon Willisonâ€™s Weblog Subscribe ...   \n",
              "5   [<1-hop>\\n\\nacknowledge that. If you tell me t...   \n",
              "6   [<1-hop>\\n\\nthis. LLMs need better criticism A...   \n",
              "7   [<1-hop>\\n\\nSimon Willisonâ€™s Weblog Subscribe ...   \n",
              "8   [<1-hop>\\n\\nThings we learned about LLMs in 20...   \n",
              "9   [<1-hop>\\n\\nSimon Willisonâ€™s Weblog Subscribe ...   \n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "10  [<1-hop>\\n\\nSimon Willisonâ€™s Weblog Subscribe ...   \n",
              "11  [<1-hop>\\n\\nSimon Willisonâ€™s Weblog Subscribe ...   \n",
              "\n",
              "                                            reference  \\\n",
<<<<<<< HEAD
              "0   In February, Meta released Llama, and in July,...   \n",
              "1   In September last year, the term 'prompt injec...   \n",
              "2   Simon Willison's Weblog highlights that 2023 w...   \n",
              "3   Stanford Alpaca is associated with the acceler...   \n",
              "4   The ethical concerns associated with the gulli...   \n",
              "5   Large Language Models (LLMs) are considered bl...   \n",
              "6   The ethical concerns related to the gullibilit...   \n",
              "7   The ethics of AI and the gullibility of langua...   \n",
              "8   In 2024, the development of models surpassing ...   \n",
              "9   DeepSeek v3, a 685B parameter model, is one of...   \n",
              "10  In 2024, the breaking of the GPT-4 barrier sig...   \n",
              "11  The introduction of GPT-4o has significantly i...   \n",
=======
              "0   According to the provided context, EleutherAI ...   \n",
              "1   In 2023, AI refers to Large Language Models (L...   \n",
              "2   The context mentions OpenAI as one of the top ...   \n",
              "3   The context does not provide specific informat...   \n",
              "4   The document discusses how effective AI model ...   \n",
              "5   The context highlights that the dependence of ...   \n",
              "6   The context emphasizes that the most critical ...   \n",
              "7   Recent advancements in Large Language Models (...   \n",
              "8   In 2024, Google made significant strides in la...   \n",
              "9   Simon Willisonâ€™s weblog highlights that 2023 w...   \n",
              "10  The context indicates that 2023 was a breakthr...   \n",
              "11  The context highlights that Appleâ€™s MLX librar...   \n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
<<<<<<< HEAD
          "execution_count": 22,
=======
          "execution_count": 20,
>>>>>>> d2a6684 (Updated Assignment 7)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSRr2MXk0P_"
      },
      "source": [
        "We'll need to provide our LangSmith API key, and set tracing to \"true\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLDUsLJg43k7"
      },
      "source": [
        "# ğŸ¤ BREAKOUT ROOM #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SLtk1GtnyoY"
      },
      "source": [
        "## Task 4: LangSmith Dataset\n",
        "\n",
        "Now we can move on to creating a dataset for LangSmith!\n",
        "\n",
        "First, we'll need to create a dataset on LangSmith using the `Client`!\n",
        "\n",
        "We'll name our Dataset to make it easy to work with later."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 14,
=======
      "execution_count": 23,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "TLgm6OjvYSsm"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"State of AI Across the Years _ Homework Attempt 2!\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"State of AI Across the Years!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64SmXMBnzXWm"
      },
      "source": [
        "We'll iterate through the RAGAS created dataframe - and add each example to our created dataset!\n",
        "\n",
        "> NOTE: We need to conform the outputs to the expected format - which in this case is: `question` and `answer`."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 15,
=======
      "execution_count": 24,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "8nFQ6di_XnY7"
      },
      "outputs": [],
      "source": [
        "for data_row in dataset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6EbQVyZq-2j"
      },
      "source": [
        "## Basic RAG Chain\n",
        "\n",
        "Time for some RAG!\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 16,
=======
      "execution_count": 25,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "4njbUAIsaYjB"
      },
      "outputs": [],
      "source": [
        "rag_documents = docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQorBy8H1AZR"
      },
      "source": [
        "To keep things simple, we'll just use LangChain's recursive character text splitter!\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 17,
=======
      "execution_count": 26,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "qWo3Ajaragv1"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "rag_documents = text_splitter.split_documents(rag_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kghuTb9R01oO"
      },
      "source": [
        "We'll create our vectorstore using OpenAI's [`text-embedding-3-small`](https://platform.openai.com/docs/guides/embeddings/embedding-models) embedding model."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 18,
=======
      "execution_count": 27,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "UwfJCzP3aqKI"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpCLS-a01Ft2"
      },
      "source": [
        "As usual, we will power our RAG application with Qdrant!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 19,
=======
      "execution_count": 28,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "58Ypj_NgbEsi"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"State of AI\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 20,
=======
      "execution_count": 29,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "SbKSjfSkbTYo"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxUOMaQX1K2N"
      },
      "source": [
        "To get the \"A\" in RAG, we'll provide a prompt."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 21,
=======
      "execution_count": 30,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "1sLeY1oWbVqO"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZnHDh4e1Ou5"
      },
      "source": [
        "For our LLM, we will be using TogetherAI's endpoints as well!\n",
        "\n",
        "We're going to be using Meta Llama 3.1 70B Instruct Turbo - a powerful model which should get us powerful results!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 22,
=======
      "execution_count": 31,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "6nx-ue1XbciV"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmTL6-pc1ZGz"
      },
      "source": [
        "Finally, we can set-up our RAG LCEL chain!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 23,
=======
      "execution_count": 32,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "TjWj0OLIbbFc"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | rag_prompt | llm | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 24,
=======
      "execution_count": 33,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WQ7bEweo4IIb",
        "outputId": "d161b269-f799-4920-d6ce-c202f6e783aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "'Based on the provided context, \"agents\" is an infuriatingly vague and imprecise term within AI that lacks a single, clear, and widely understood meaning. The term generally seems to converge on the idea of \"AI systems that can go away and act on your behalf.\" There are two main interpretations: one views agents as systems that act on your behalf in a manner similar to a travel agent, and the other sees them as large language models (LLMs) given access to tools which they run in loops to solve problems. Despite much discussion, actual functioning AI agents in production remain rare or nonexistent, partly due to challenges like gullibility in LLMs, which believe anything they are told and thus struggle to reliably make meaningful decisions on a user\\'s behalf.\\n\\nIn summary, \"agents\" refer broadly to AI systems designed to perform tasks autonomously on behalf of users, but the concept remains vague, under-defined, and largely unrealized in practice.'"
            ]
          },
          "execution_count": 24,
=======
              "'Based on the provided context, \"agents\" is an infuriatingly vague and unclear term in the AI community. Generally, it refers to AI systems that can go away and act on your behalf, often imagined as digital assistants or travel-agent-like models. Another interpretation involves large language models (LLMs) given access to tools which they can run iteratively to solve problems. However, there is no single, clear, or widely understood definition of \"agents,\" and their practical, real-world deployment remains limited, partly due to challenges like gullibility in AI systems. Overall, agents are conceptually AI entities intended to act autonomously for users, but the exact meaning varies and remains unsettled.'"
            ]
          },
          "execution_count": 33,
>>>>>>> d2a6684 (Updated Assignment 7)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"question\" : \"What are Agents?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9hBh5YPrdGJ"
      },
      "source": [
        "## LangSmith Evaluation Set-up\n",
        "\n",
        "We'll use OpenAI's GPT-4.1 as our evaluation LLM for our base Evaluators."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 25,
=======
      "execution_count": 34,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "gfwPYdIkcvpF"
      },
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b8pToKH2K28"
      },
      "source": [
        "We'll be using a number of evaluators - from LangSmith provided evaluators, to a few custom evaluators!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 26,
=======
      "execution_count": 35,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "PXSG-_ajckp6"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
        "\n",
        "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"helpfulness\": (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \" taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"output\"],\n",
        "        \"reference\": example.outputs[\"answer\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "dope_or_nope_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"dopeness\": \"Is this submission dope, lit, or cool?\",\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0SQP_FoCetP"
      },
      "source": [
        "#### ğŸ—ï¸ Activity #2:\n",
        "\n",
        "Highlight what each evaluator is evaluating.\n",
        "\n",
<<<<<<< HEAD
        "- `qa_evaluator`:\n",
        "- `labeled_helpfulness_evaluator`:\n",
        "- `dope_or_nope_evaluator`:"
=======
        "- `qa_evaluator`: LangChain's evaluation module provides evaluators you can use as-is for common evaluation scenarios. qa_evaluator is one of the out-of-the-box ones. It outputs correctness of answer. i.e accuracy . Uses question + model prediction + reference answer.\n",
        "- `labeled_helpfulness_evaluator`: This is another out of the box evaluator. The output Depends on criteria key, here we use helpfulness criteria( (semantic alignment + usefulness), which is a default implemented criteria. It Measures how helpful the model's output is, based on a labeled reference answer. Other criteria include conciseness, relevance, correctness, coherence, harmfulness, maliciousness. Or you can define your own criteria. \n",
        "helpfulness uses \n",
        "â€¢ input = question\n",
        "â€¢ prediction = model answer\n",
        "â€¢ reference = correct answer\n",
        "- `dope_or_nope_evaluator`: It is using custom criteria that is Subjective, creative-style evaluator asking whether the output is cool or impressive.Uses only model output. "
>>>>>>> d2a6684 (Updated Assignment 7)
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R35sQMHVrnpl"
      },
      "source": [
        "## LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 27,
=======
      "execution_count": 36,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "122b1bd1f0e9417a8dcb57d4eebe4d2e",
            "e0c233ad01604540a6c873f4a731982d",
            "e9a01115c75b499884f7e0ef32e9e599",
            "5faba4ad609448b2b49024add4ad3b8e",
            "ef25efa751304e4699910f1fbc14345f",
            "0b44cb0f8e34446c8dde668a75d3d8ad",
            "edaac6587b2d4bd5be52b89bb097f99f",
            "7cb241365f604419af454c1c28de197a",
            "9cf586576ff44dba86ba2eb389593c61",
            "849b5c95008541d49f1ceedf0a59ac60",
            "f3665a86662746c4ac7cb0796604781d"
          ]
        },
        "id": "t7t_Uz0tdumL",
        "outputId": "d684e218-294e-4dc3-c8de-a01d397f021c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "View the evaluation results for experiment: 'timely-summer-45' at:\n",
            "https://smith.langchain.com/o/340cd80b-3296-5752-9a9e-58582118073a/datasets/fcd16694-8364-42d5-82dc-7d101a744f77/compare?selectedSessions=d84ec125-928b-441e-b009-b680c123a95a\n",
=======
            "View the evaluation results for experiment: 'spotless-reason-69' at:\n",
            "https://smith.langchain.com/o/4e223e9d-b789-4c00-8d16-32ad70974f10/datasets/47b2c106-3f8c-45b4-80e7-b824cdc3256d/compare?selectedSessions=062e76a1-075c-4a06-9608-356ad1e41d18\n",
>>>>>>> d2a6684 (Updated Assignment 7)
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "9d629a3532ae429f93b7e3baa79abb35",
=======
              "model_id": "ae925a356c5b44699c3fea941ec11e4d",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.correctness</th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.dopeness</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
<<<<<<< HEAD
              "      <td>are LLMs smart dumb how they work</td>\n",
              "      <td>Based on the provided context, LLMs (Large Lan...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context explains that LLMs are really smar...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.681994</td>\n",
              "      <td>3ff22830-4121-4f92-97e4-69038fc547f4</td>\n",
              "      <td>3fe3dff7-a98e-47f3-b699-187b7415d52b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>how LLMs are really smart but also dumb and gu...</td>\n",
              "      <td>Based on the context provided:\\n\\nLLMs (Large ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context explains that LLMs are both really...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10.806808</td>\n",
              "      <td>db606450-1257-4989-ae57-247455cbd552</td>\n",
              "      <td>b81b4652-d74a-4ffc-8559-1bbb13e82074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the development and capabilities of m...</td>\n",
              "      <td>Based on the provided context, models like Cla...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that models like Claude...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5.661992</td>\n",
              "      <td>f80f0e21-a6e6-4c39-9d70-22134d131d9b</td>\n",
              "      <td>67bded07-7b16-4799-914d-0f8e0a8c8014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do ChatGPT and other LLMs demonstrate both...</td>\n",
              "      <td>ChatGPT and other LLMs demonstrate their poten...</td>\n",
              "      <td>None</td>\n",
              "      <td>The provided context highlights that ChatGPT a...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.398812</td>\n",
              "      <td>5da6a5ad-70ea-47b5-b23b-0857efba0b18</td>\n",
              "      <td>de897e35-21b8-472a-94f1-55e375f79493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does the development of large language mod...</td>\n",
              "      <td>According to Simon Willisonâ€™s 2023 blog post, ...</td>\n",
              "      <td>None</td>\n",
              "      <td>Simon Willisonâ€™s 2023 blog post highlights tha...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4.582133</td>\n",
              "      <td>e3ffde34-9a3a-4ca8-a778-0652375e0e59</td>\n",
              "      <td>4aa7edaa-0940-4f09-b9cb-ff01d82d3e44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>how AI models impact environment and ethics of...</td>\n",
              "      <td>Based on the provided context:\\n\\n**Environmen...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context discusses the environmental impact...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5.128811</td>\n",
              "      <td>95fe690c-21b7-4aa7-96c6-372bf353a2d7</td>\n",
              "      <td>58c948c0-3006-44ce-839f-20e6b8794eb8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Considering the regional development of large ...</td>\n",
              "      <td>Based on the provided context, advancements in...</td>\n",
=======
              "      <td>Based on the insights about Appleâ€™s MLX librar...</td>\n",
              "      <td>Based on the provided context, Appleâ€™s approac...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that Appleâ€™s MLX librar...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>9.254166</td>\n",
              "      <td>b4a68595-c6c6-4a0f-b000-626e1e621a7a</td>\n",
              "      <td>e361ac57-0ddc-41f0-8f72-db5771225344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>H0w do LLMs sh0w the progress in AI in 2023 an...</td>\n",
              "      <td>Based on the provided context:\\n\\nLLMs demonst...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context indicates that 2023 was a breakthr...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5.629479</td>\n",
              "      <td>4f8557f6-cea9-4ac6-b34c-e90358edd5c1</td>\n",
              "      <td>47a2e140-f7aa-4d06-999a-2398741ba8d1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Based on Simon Willisonâ€™s insights, how do the...</td>\n",
              "      <td>Based on Simon Willisonâ€™s insights from the pr...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>None</td>\n",
              "      <td>In 2024, the regional development of large lan...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
<<<<<<< HEAD
              "      <td>8.017292</td>\n",
              "      <td>8d07f816-6659-411f-a86a-e283727521d3</td>\n",
              "      <td>e242745c-c564-40ce-a5be-2c0139e6b057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>how does the evolution of AI models from 2023 ...</td>\n",
              "      <td>The context draws an interesting historical pa...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context shows that in 2024, AI models have...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.356398</td>\n",
              "      <td>4ad32332-981d-444d-b8a5-6873f72f5a97</td>\n",
              "      <td>68682280-481a-4f80-ba83-10078174ca6e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>what is stanford alpaca</td>\n",
              "      <td>I don't know</td>\n",
              "      <td>None</td>\n",
              "      <td>The context does not provide a specific explan...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.306795</td>\n",
              "      <td>5cd3e64c-ac3f-4331-9b22-8f4185377410</td>\n",
              "      <td>b1b77ceb-a144-43db-b794-3998fab46388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are the key insights about LLMs highlight...</td>\n",
              "      <td>The key insights about LLMs highlighted by Sim...</td>\n",
              "      <td>None</td>\n",
              "      <td>Simon Willisonâ€™s weblog notes that 2023 was th...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.997499</td>\n",
              "      <td>e7aba49b-c3b1-404f-bf75-44c872296a30</td>\n",
              "      <td>9622b547-e4d5-4372-b06d-6ef9642c5dbb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What is OpenAI known for in AI research?</td>\n",
              "      <td>Based on the provided context, OpenAI is known...</td>\n",
              "      <td>None</td>\n",
              "      <td>The provided context does not specify what Ope...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.389238</td>\n",
              "      <td>aa4dd8f3-9567-4ed9-853b-30e131c80813</td>\n",
              "      <td>1131d8de-ebe9-4d83-b1bf-31d577c69d99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Google is like what in AI?</td>\n",
              "      <td>Based on the context, Google is one of the lea...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context mentions that Google is one of the...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4.216128</td>\n",
              "      <td>2f96b0be-54b3-4249-8722-d6c07934ba93</td>\n",
              "      <td>2dd98d4e-303c-4a1e-8eb8-83a32048c08d</td>\n",
=======
              "      <td>5.877384</td>\n",
              "      <td>43eeeb7a-d943-46da-8219-bfdedddf796a</td>\n",
              "      <td>2d8685bd-bb09-470f-9b5c-bbbdcbd1cf70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How does Google's recent advancements in large...</td>\n",
              "      <td>Based on the provided context, Google's Gemini...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2024, Google made significant strides in la...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8.948981</td>\n",
              "      <td>cf45d9a6-7a45-4dbb-879e-fb9db1a49d16</td>\n",
              "      <td>722bd858-f4b1-4d34-b173-53a831c444e5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the recent advancements in Large Langua...</td>\n",
              "      <td>Recent advancements in Large Language Models (...</td>\n",
              "      <td>None</td>\n",
              "      <td>Recent advancements in Large Language Models (...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4.675432</td>\n",
              "      <td>05d67017-1262-4a68-8cb4-0f967b6753be</td>\n",
              "      <td>3ccb23f2-f989-460f-b30c-e952f73492e1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How does the importance of training data quali...</td>\n",
              "      <td>Based on the provided context, the quality and...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context emphasizes that the most critical ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.661058</td>\n",
              "      <td>6e97d914-1b07-43b9-a9cf-be828d739f83</td>\n",
              "      <td>eb7b0d50-10af-4c7c-8d70-d5fb9ecf0431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the dependence of agent capabilities ...</td>\n",
              "      <td>The context suggests that current AI agents' c...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that the dependence of ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5.315406</td>\n",
              "      <td>b91c472d-bbe9-4ea8-b391-52b6dfc8a94e</td>\n",
              "      <td>aa2444be-5df9-4c89-8d15-e9b4d4b1caca</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How do AI model release strategies, like Qwen ...</td>\n",
              "      <td>Based on the provided context, AI model releas...</td>\n",
              "      <td>None</td>\n",
              "      <td>The document discusses how effective AI model ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.819824</td>\n",
              "      <td>205f90f1-94b5-439c-9f16-6d2e2523267e</td>\n",
              "      <td>78f8a1ba-a22f-4b1c-a292-2f4258f4ebac</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wha is OpenAI's role in the development of lar...</td>\n",
              "      <td>Based on the provided context, OpenAI played a...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context does not provide specific informat...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.735545</td>\n",
              "      <td>4bd0440b-57d5-4629-917f-2115ac23ed41</td>\n",
              "      <td>d539adb0-9809-4d2b-8ad9-711d689980ba</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What is OpenAI in context of large language mo...</td>\n",
              "      <td>OpenAI is an organization that, a year ago, ha...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context mentions OpenAI as one of the top ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.272184</td>\n",
              "      <td>108bd2c7-c4bd-4852-9037-2f20d940df5d</td>\n",
              "      <td>e6bd0135-8337-4ebc-8375-f4cc361ce221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>AI is what in 2023?</td>\n",
              "      <td>In 2023, AI, specifically Large Language Model...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2023, AI refers to Large Language Models (L...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.035894</td>\n",
              "      <td>677f6b23-933e-4e48-9c05-6b633df55872</td>\n",
              "      <td>c9bc9184-abff-47f5-8e0b-298995ab60fc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What is EleutherAI and how does it relate to t...</td>\n",
              "      <td>Based on the provided context, EleutherAI is o...</td>\n",
              "      <td>None</td>\n",
              "      <td>According to the provided context, EleutherAI ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.012946</td>\n",
              "      <td>bd7cc79a-f935-4512-bafe-41f38ce75371</td>\n",
              "      <td>cdf9c3ac-6376-42a7-a9d5-337c38f86cd1</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
<<<<<<< HEAD
              "<ExperimentResults timely-summer-45>"
            ]
          },
          "execution_count": 27,
=======
              "<ExperimentResults spotless-reason-69>"
            ]
          },
          "execution_count": 36,
>>>>>>> d2a6684 (Updated Assignment 7)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        dope_or_nope_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq7fCVinrpI4"
      },
      "source": [
        "## Dope-ifying Our Application\n",
        "\n",
        "We'll be making a few changes to our RAG chain to increase its performance on our SDG evaluation test dataset!\n",
        "\n",
        "- Include a \"dope\" prompt augmentation\n",
        "- Use larger chunks\n",
        "- Improve the retriever model to: `text-embedding-3-large`\n",
        "\n",
        "Let's see how this changes our evaluation!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 61,
=======
      "execution_count": 37,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "z56pXwyUgFUt"
      },
      "outputs": [],
      "source": [
        "DOPE_RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "You must answer the questions in a dope way, be cool!\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "dope_rag_prompt = ChatPromptTemplate.from_template(DOPE_RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 62,
=======
      "execution_count": 38,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "rZLcTstJgfv5"
      },
      "outputs": [],
      "source": [
        "rag_documents = docs"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 63,
=======
      "execution_count": 39,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "-LYsyirngj6n"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "rag_documents = text_splitter.split_documents(rag_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spldiPuTCzDO"
      },
      "source": [
        "#### â“Question #2:\n",
        "\n",
        "Why would modifying our chunk size modify the performance of our application?\n",
        "\n",
        "The chunk size controls how much content each chunk holds, usually measured in tokens or characters. Chunk size affects the answers accuracy, retrieval relevance, the latency and performance, affects token window management and semantic chunk boundaries (repeat vs exclusive content)\n",
        "\n",
        "here we increse the chunk size to 1000 from 500,while keeping chunk_overlap the same. Larger chunks = bigger retrieval payloads = longer LLM context windows needed, means also increased token size and inference costs. Since most models have limited token size, Large chunks can crowd out other relevant chunks if context space is limited. Too-large chunks may introduce distractors, hurting the factuality of the generated output. However, larger chunks are Better for semantic cohesion and multi-sentence reasoning, but run the Risk of including irrelevant content (semantic drift). In other words, Chunk size directly modulates the precision, completeness, and efficiency of retrieval â€” and thus the factuality and helpfulness of our RAG outputs."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 64,
=======
      "execution_count": 40,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "b9MI2Bm2go1r"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBbjG6cKC8BQ"
      },
      "source": [
        "#### â“Question #3:\n",
        "\n",
        "Why would modifying our embedding model modify the performance of our application?\n",
        "\n",
        "Embedding model transforms the data/content into numerical vector representation, that we then extract the chunks during retrival phase. The entire retrieval performance hinges on the embedding modelâ€™s ability to semantically match inputs. \n",
        "1) Semantic similarity : A weak embedding model will Poorly capture of meaning â†’ irrelevant chunks. While  Strong Embedding Model leads to Precise chunk matching\n",
        "2) Recall of relevant docs: A weak model Misses key info, while strong model has High precision + recall\n",
        "3) Multi-hop reasoning: A weak mode has Low signal chaining, while a good model has Better chain coherence\n",
        "4) Context ranking :A weak model has noisy results. strong model focuses on top-k retrieval\n",
        "5) Downstream LLM quality:  A weak LLM gets wrong context â†’ hallucinations exist. While a good LLM grounded in right context has better factual accuracy and faithfulness. "
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 65,
=======
      "execution_count": 41,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "hVUY25FKgxXx"
      },
      "outputs": [],
      "source": [
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"AI Across Years (Augmented)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 66,
=======
      "execution_count": 42,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "Q4TOZNYIg2v1"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqYGFrnKDB91"
      },
      "source": [
        "Setting up our new and improved DOPE RAG CHAIN."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 67,
=======
      "execution_count": 43,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "id": "HqnTqeXMhAdx"
      },
      "outputs": [],
      "source": [
        "dope_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | dope_rag_prompt | llm | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21pTxoqJDI1Y"
      },
      "source": [
        "Let's test it on the same output that we saw before."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 68,
=======
      "execution_count": 44,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OfZZ3MoN3fKv",
        "outputId": "d65722dd-92c2-4e4e-9cca-c42ee6f3f208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "'Agents? Man, theyâ€™re this super vague concept in the AI world. Some folks think of them as digital assistants that act on your behalfâ€”like a travel agent. Others see them as AI models with tools, looping through tasks to solve problems. But hereâ€™s the kicker: the term is so fuzzy that it leaves you scratching your head, since everyone seems to have their own take on what it really means. Plus, thereâ€™s this whole issue of gullibilityâ€”how can these agents make smart choices if they canâ€™t tell reality from fiction? So yeah, theyâ€™re like this elusive dream still waiting for a real breakthrough. ğŸ’«'"
            ]
          },
          "execution_count": 68,
=======
              "\"Yo, so hereâ€™s the lowdown on â€œagentsâ€ straight from the AI street: the term â€œagentsâ€ is hella vague and kinda frustrating 'cause nobodyâ€™s really nailed down what it *actually* means. Some folks see agents like your classic travel agentâ€”something that goes out and acts on your behalf. Others think of agents as LLMs strapped with tools, running in loops to crack problems. But here's the kicker: these agents arenâ€™t really hitting the scene yet, mostly â€˜cause they fall prey to gullibilityâ€”like they just trust anything thrown their way.\\n\\nSo, agents are basically AI systems that *should* act autonomously for you, but the techâ€™s still stumbling on the truth-detection grind, meaning you gotta wait a bit longer before you get a legit, dependable AI sidekick. Theyâ€™re â€œcoming soon,â€ but not quite here to sling dope moves just yet.\\n\\nStay tuned, â€˜cause until these models get their skepticism game tightâ€”like an AGI-level hustleâ€”weâ€™re stuck in agent limbo. Peace!\""
            ]
          },
          "execution_count": 44,
>>>>>>> d2a6684 (Updated Assignment 7)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dope_rag_chain.invoke({\"question\" : \"what are Agents?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpj7v1inDLnQ"
      },
      "source": [
        "Finally, we can evaluate the new chain on the same test set!"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 69,
=======
      "execution_count": 45,
>>>>>>> d2a6684 (Updated Assignment 7)
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "bf8dcc0895054529af356da401c513f6",
            "7dce19ac55264f2b88a0e4730e55867b",
            "2a0755d4476543feb4a64538e3e37213",
            "158212a630f04cbd884c937f2f60f5c8",
            "11c7f66acc1d45be9517d0addf49331e",
            "ddffd834e09940a4bd3874c3f39b4e21",
            "ef63c3b2d51e452da03cdae5d9b034be",
            "c20b539cd70b4ba99601ad1d69fd9cec",
            "a6d681eeafa44d18b933a4c5dec88382",
            "d1d54ccd56494c4d831f71b416a1f880",
            "530f696feefe499da08c6312047379b2"
          ]
        },
        "id": "Dx11S2b-hIM8",
        "outputId": "d3a3ea78-aa32-4bd2-8c2a-d0d0303695c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "View the evaluation results for experiment: 'artistic-airplane-98' at:\n",
            "https://smith.langchain.com/o/117cfda3-8a09-4ba4-9922-07b45fd73803/datasets/25fa804e-0ce3-4848-9cd1-c83d91988e78/compare?selectedSessions=42ea0e06-22a8-4bfb-a5bc-760a0cf4d280\n",
=======
            "View the evaluation results for experiment: 'ample-stew-18' at:\n",
            "https://smith.langchain.com/o/4e223e9d-b789-4c00-8d16-32ad70974f10/datasets/47b2c106-3f8c-45b4-80e7-b824cdc3256d/compare?selectedSessions=f27d4ee2-021b-48d5-97bb-773ef0c88729\n",
>>>>>>> d2a6684 (Updated Assignment 7)
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
              "model_id": "f72d571a85c4495db152af05ef004ffd",
=======
              "model_id": "f4ccd24df18b44b98863776385df17f5",
>>>>>>> d2a6684 (Updated Assignment 7)
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.correctness</th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.dopeness</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
<<<<<<< HEAD
              "      <td>How has the introduction of GPT-4o impacted th...</td>\n",
              "      <td>Yo, the rollout of GPT-4o is such a game chang...</td>\n",
              "      <td>None</td>\n",
              "      <td>The introduction of GPT-4o has significantly i...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.590299</td>\n",
              "      <td>e749dc1c-f46a-4e61-9742-daa3d275671c</td>\n",
              "      <td>dffab390-a87b-4d08-93a3-8eedc7d24a7e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How did the breaking of the GPT-4 barrier in 2...</td>\n",
              "      <td>Yo, the breaking of the GPT-4 barrier in 2024 ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2024, the breaking of the GPT-4 barrier sig...</td>\n",
=======
              "      <td>Based on the insights about Appleâ€™s MLX librar...</td>\n",
              "      <td>Alright, hereâ€™s the lowdown, fresh and fly:\\n\\...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that Appleâ€™s MLX librar...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6.747858</td>\n",
              "      <td>b4a68595-c6c6-4a0f-b000-626e1e621a7a</td>\n",
              "      <td>67c418ce-3e15-4312-ba30-60569b710da5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>H0w do LLMs sh0w the progress in AI in 2023 an...</td>\n",
              "      <td>Aight, hereâ€™s the lowdown on LLMs flexinâ€™ the ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context indicates that 2023 was a breakthr...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>1</td>\n",
              "      <td>1</td>\n",
<<<<<<< HEAD
              "      <td>3.217965</td>\n",
              "      <td>e0f5c6c4-fb8e-411e-8399-95212616c92b</td>\n",
              "      <td>d10800e7-981e-4127-8641-d2f357f52723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the training efficiency of DeepSeek v...</td>\n",
              "      <td>Yo, check it! DeepSeek v3 is flexing some seri...</td>\n",
              "      <td>None</td>\n",
              "      <td>DeepSeek v3, a 685B parameter model, is one of...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.265255</td>\n",
              "      <td>28816b79-8c83-49b7-90f3-dc5a99ee6a7f</td>\n",
              "      <td>cddb24bd-3a3c-4ad3-9f10-cebb03122d05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How has the development of models surpassing G...</td>\n",
              "      <td>Yo, the scene in 2024 is pretty wild! With 18 ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2024, the development of models surpassing ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.281109</td>\n",
              "      <td>757f3ec9-c32f-4d24-a2f7-d02b88fdb499</td>\n",
              "      <td>2fa50d79-687c-4902-a08c-9e603a4bab64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the ethics of AI and the gullibility of...</td>\n",
              "      <td>Yo, the ethics of AI and the gullibility of la...</td>\n",
              "      <td>None</td>\n",
              "      <td>The ethics of AI and the gullibility of langua...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.492535</td>\n",
              "      <td>bf0ac2b1-9738-4e30-8f8b-1b5d9e8c3b04</td>\n",
              "      <td>56695a52-31b0-4ce8-bd7f-f071a3649b82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What are the ethical concerns related to the g...</td>\n",
              "      <td>Yo, the ethical vibes around the gullibility o...</td>\n",
              "      <td>None</td>\n",
              "      <td>The ethical concerns related to the gullibilit...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.733880</td>\n",
              "      <td>d312a7f8-fd86-4df7-9d4d-6f9387a11412</td>\n",
              "      <td>32fe5cb2-0df4-4265-ad76-0487a7cbcb9f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Why are Large Language Models (LLMs) considere...</td>\n",
              "      <td>LLMs are considered black boxes because, despi...</td>\n",
=======
              "      <td>1</td>\n",
              "      <td>8.790863</td>\n",
              "      <td>4f8557f6-cea9-4ac6-b34c-e90358edd5c1</td>\n",
              "      <td>eb691afb-6f67-4f67-a80a-00a209c8d516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Based on Simon Willisonâ€™s insights, how do the...</td>\n",
              "      <td>Yo, hereâ€™s the lowdown from Simon Willisonâ€™s s...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>None</td>\n",
              "      <td>Large Language Models (LLMs) are considered bl...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
<<<<<<< HEAD
              "      <td>7.673780</td>\n",
              "      <td>28659332-3637-4cee-9018-98d587575f5b</td>\n",
              "      <td>6d4eece2-36ce-45e1-b308-11ab50b14f34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the ethical concerns associated with ...</td>\n",
              "      <td>Yo, the ethical vibes surrounding the gullibil...</td>\n",
              "      <td>None</td>\n",
              "      <td>The ethical concerns associated with the gulli...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>15.375634</td>\n",
              "      <td>2da5c189-eb9e-419b-9c93-e3247d9f154c</td>\n",
              "      <td>6b2701ac-8dd9-42bc-b4cf-92c410f43390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What role does Stanford Alpaca play in the dev...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>Stanford Alpaca is associated with the acceler...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.234988</td>\n",
              "      <td>1c5369b1-7b7e-476d-9d28-816ce1a360f6</td>\n",
              "      <td>4be48806-dee2-4810-9d3a-6a07e97d14ac</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Whaat are the key highlights from Simon Willis...</td>\n",
              "      <td>Yo, letâ€™s break it down! In 2023, Simon Willis...</td>\n",
              "      <td>None</td>\n",
              "      <td>Simon Willison's Weblog highlights that 2023 w...</td>\n",
=======
              "      <td>7.407839</td>\n",
              "      <td>43eeeb7a-d943-46da-8219-bfdedddf796a</td>\n",
              "      <td>4023ddd5-029b-423f-80ec-08a55fa2dbe7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How does Google's recent advancements in large...</td>\n",
              "      <td>Alright, hereâ€™s the lowdown, straight fire sty...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2024, Google made significant strides in la...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.081409</td>\n",
              "      <td>cf45d9a6-7a45-4dbb-879e-fb9db1a49d16</td>\n",
              "      <td>9b7403df-1675-44c5-9557-bbe97271a584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the recent advancements in Large Langua...</td>\n",
              "      <td>Yo, hereâ€™s the lowdownâ€”LLMs are flexing hard i...</td>\n",
              "      <td>None</td>\n",
              "      <td>Recent advancements in Large Language Models (...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.230204</td>\n",
              "      <td>05d67017-1262-4a68-8cb4-0f967b6753be</td>\n",
              "      <td>cc715e10-6429-4f40-9995-fbfec322e3b2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How does the importance of training data quali...</td>\n",
              "      <td>Aight, hereâ€™s the lowdownâ€”training data qualit...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context emphasizes that the most critical ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.570817</td>\n",
              "      <td>6e97d914-1b07-43b9-a9cf-be828d739f83</td>\n",
              "      <td>245510d9-24eb-4da1-81ba-a02597eeb3b4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the dependence of agent capabilities ...</td>\n",
              "      <td>Alright, so hereâ€™s the lowdown: The power of A...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that the dependence of ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.243256</td>\n",
              "      <td>b91c472d-bbe9-4ea8-b391-52b6dfc8a94e</td>\n",
              "      <td>3acfff67-e9a6-43ca-abd8-b1dbde4359bf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How do AI model release strategies, like Qwen ...</td>\n",
              "      <td>Yo, check itâ€”based on the info at hand, the re...</td>\n",
              "      <td>None</td>\n",
              "      <td>The document discusses how effective AI model ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6.354277</td>\n",
              "      <td>205f90f1-94b5-439c-9f16-6d2e2523267e</td>\n",
              "      <td>abae310a-76f6-47ba-8a0a-ee819b93ffe9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wha is OpenAI's role in the development of lar...</td>\n",
              "      <td>Yo, based on the vibe from the context, OpenAI...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context does not provide specific informat...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.834875</td>\n",
              "      <td>4bd0440b-57d5-4629-917f-2115ac23ed41</td>\n",
              "      <td>eab34bb5-87b1-4691-8cda-4ce2b96c3394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What is OpenAI in context of large language mo...</td>\n",
              "      <td>Alright, hereâ€™s the lowdown on OpenAI in the l...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context mentions OpenAI as one of the top ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3.307975</td>\n",
              "      <td>108bd2c7-c4bd-4852-9037-2f20d940df5d</td>\n",
              "      <td>25dc17c5-9c3a-48fe-8215-714275d79fd2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>AI is what in 2023?</td>\n",
              "      <td>Yo, in 2023, AI was basically leveled up with ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In 2023, AI refers to Large Language Models (L...</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "      <td>1</td>\n",
              "      <td>1</td>\n",
<<<<<<< HEAD
              "      <td>5.724408</td>\n",
              "      <td>012f7bdc-73f0-4ad8-8a30-e89edbb0a0e7</td>\n",
              "      <td>2c0cd907-d3ed-452c-8bac-e5b3368a1980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What significant event related to AI ethics oc...</td>\n",
              "      <td>Yo, in September last year, there was a major ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In September last year, the term 'prompt injec...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.863023</td>\n",
              "      <td>5719d5a1-4355-41b4-aabc-dd1c10357462</td>\n",
              "      <td>13719abd-f8cb-4d31-bf90-bb21317be497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Wut has Meta dun in the feeld of LLMs this yeer?</td>\n",
              "      <td>Yo, Meta's been making waves in the LLM scene ...</td>\n",
              "      <td>None</td>\n",
              "      <td>In February, Meta released Llama, and in July,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.654334</td>\n",
              "      <td>dcc029aa-596c-44f0-89c4-c54fa92184ea</td>\n",
              "      <td>e319edf2-130c-4379-ac89-62a205a21ad0</td>\n",
=======
              "      <td>1</td>\n",
              "      <td>4.838505</td>\n",
              "      <td>677f6b23-933e-4e48-9c05-6b633df55872</td>\n",
              "      <td>f96d91a6-42af-4e89-a66f-4e7d8742ced2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What is EleutherAI and how does it relate to t...</td>\n",
              "      <td>Alright, hereâ€™s the lowdown: EleutherAI is one...</td>\n",
              "      <td>None</td>\n",
              "      <td>According to the provided context, EleutherAI ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.211585</td>\n",
              "      <td>bd7cc79a-f935-4512-bafe-41f38ce75371</td>\n",
              "      <td>4fc30886-782a-4e45-a553-4055bec322a9</td>\n",
>>>>>>> d2a6684 (Updated Assignment 7)
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
<<<<<<< HEAD
              "<ExperimentResults artistic-airplane-98>"
            ]
          },
          "execution_count": 69,
=======
              "<ExperimentResults ample-stew-18>"
            ]
          },
          "execution_count": 45,
>>>>>>> d2a6684 (Updated Assignment 7)
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    dope_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        dope_or_nope_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"dope_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C7migvlDPZT"
      },
      "source": [
        "#### ğŸ—ï¸ Activity #3:\n",
        "\n",
        "Provide a screenshot of the difference between the two chains, and explain why you believe certain metrics changed in certain ways.\n",
        "\n",
        "\n",
        "Answer\n",
        "We noted that all three metrics improved with the lastest run, specifically dopeness with the most improvement. We used a better model(text-embedding-3-large (OpenAIâ€™s top-tier dense retriever)) , larger chunk size and defined Dope augmentation more specifically. \n",
        "\n",
        "1) Correctness: Improved slightly, as text-embedding-3-large has state-of-the-art semantic understanding, improving top-k chunk selection. Larger chunks captured more comprehensive context, reducing hallucinations and missing info.\n",
        "\n",
        "2) Helpfulness: Improved noticeably in the second run. Dopeness prompt augmentation probably also improved clarity, tone, or added user-centric phrasing (making output feel more helpful). Larger chunks likely led to better contextual grounding, especially for abstract or longer questions. This mean we're not retriveing correct context, weâ€™re retrieving useful content and wrapping it in user-friendly responses.\n",
        "\n",
        "3) Dopeness: Jumped from 0-->1 \n",
        "This was the direct optimization target. We NUsed creative prompt augmentation to inject tone/style. Our Style shift doesnâ€™t hurt correctness because weâ€™ve improved grounding through better retrieval and larger chunks. Our prompts also were more specific for context. "
      ]
<<<<<<< HEAD
=======
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![label](Homework_Chart.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
>>>>>>> d2a6684 (Updated Assignment 7)
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07ab3dc0790241bbb85a7f488a42ef8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7710c7377cbc4c30b55b28b4bc99e88f",
              "IPY_MODEL_41bdd49fab5f4826959d0d50663ff539",
              "IPY_MODEL_60168d85131d4afc99d55d61ab954ee6"
            ],
            "layout": "IPY_MODEL_9edf898aeeab40dda9b9475395776521"
          }
        },
        "095f680d37a3430fb82d223615662db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b44cb0f8e34446c8dde668a75d3d8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10df31709059484c99f102453d780473": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1160a44dc18e47b0890f70c40eaa7eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11c7f66acc1d45be9517d0addf49331e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122b1bd1f0e9417a8dcb57d4eebe4d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0c233ad01604540a6c873f4a731982d",
              "IPY_MODEL_e9a01115c75b499884f7e0ef32e9e599",
              "IPY_MODEL_5faba4ad609448b2b49024add4ad3b8e"
            ],
            "layout": "IPY_MODEL_ef25efa751304e4699910f1fbc14345f"
          }
        },
        "158212a630f04cbd884c937f2f60f5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1d54ccd56494c4d831f71b416a1f880",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_530f696feefe499da08c6312047379b2",
            "value": "â€‡20/?â€‡[01:43&lt;00:00,â€‡â€‡5.25s/it]"
          }
        },
        "23863bc37a8645029934b8c106622c51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2508d229935744cbb5fc340222e2d660": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a0755d4476543feb4a64538e3e37213": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c20b539cd70b4ba99601ad1d69fd9cec",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6d681eeafa44d18b933a4c5dec88382",
            "value": 1
          }
        },
        "33f063017b7c4c7fa8cbafc89674350b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6864c81e2bcf459bbaf5acbb36bdfcbe",
              "IPY_MODEL_59d6e269eadf429a924f6f79bc8ba4ba",
              "IPY_MODEL_ca791fc471e34b9da2f9070fc1053c0f"
            ],
            "layout": "IPY_MODEL_8baf0ed3d0f743f294e07f2b5407e820"
          }
        },
        "3a8537e37fc14fd9b16ca0ceee4fede6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41bdd49fab5f4826959d0d50663ff539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb8b2e3262c45248708a2082c366f0a",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_095f680d37a3430fb82d223615662db5",
            "value": 64
          }
        },
        "530f696feefe499da08c6312047379b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59d6e269eadf429a924f6f79bc8ba4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_890e0dd7fa524ceca1e805cb6253ee71",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61b52ff459214129b8f7e6d67b192b78",
            "value": 20
          }
        },
        "5ab5f08afa5841709aedb2f78a52a11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c2fda99d4204d85b1bf7ad354fd58d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5faba4ad609448b2b49024add4ad3b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_849b5c95008541d49f1ceedf0a59ac60",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f3665a86662746c4ac7cb0796604781d",
            "value": "â€‡20/?â€‡[01:27&lt;00:00,â€‡â€‡6.45s/it]"
          }
        },
        "60168d85131d4afc99d55d61ab954ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8537e37fc14fd9b16ca0ceee4fede6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1160a44dc18e47b0890f70c40eaa7eb0",
            "value": "â€‡61/64â€‡[00:02&lt;00:00,â€‡23.36it/s]"
          }
        },
        "61b52ff459214129b8f7e6d67b192b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6864c81e2bcf459bbaf5acbb36bdfcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10df31709059484c99f102453d780473",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2508d229935744cbb5fc340222e2d660",
            "value": "Generating:â€‡100%"
          }
        },
        "6eb8b2e3262c45248708a2082c366f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7710c7377cbc4c30b55b28b4bc99e88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2fda99d4204d85b1bf7ad354fd58d4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_93cd4d35c5fd41f5904ca1d52d1f52a8",
            "value": "embeddingâ€‡nodes:â€‡â€‡95%"
          }
        },
        "7cb241365f604419af454c1c28de197a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7dce19ac55264f2b88a0e4730e55867b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddffd834e09940a4bd3874c3f39b4e21",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef63c3b2d51e452da03cdae5d9b034be",
            "value": ""
          }
        },
        "849b5c95008541d49f1ceedf0a59ac60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890e0dd7fa524ceca1e805cb6253ee71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8baf0ed3d0f743f294e07f2b5407e820": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93cd4d35c5fd41f5904ca1d52d1f52a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cf586576ff44dba86ba2eb389593c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9edf898aeeab40dda9b9475395776521": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a6d681eeafa44d18b933a4c5dec88382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf8dcc0895054529af356da401c513f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dce19ac55264f2b88a0e4730e55867b",
              "IPY_MODEL_2a0755d4476543feb4a64538e3e37213",
              "IPY_MODEL_158212a630f04cbd884c937f2f60f5c8"
            ],
            "layout": "IPY_MODEL_11c7f66acc1d45be9517d0addf49331e"
          }
        },
        "c20b539cd70b4ba99601ad1d69fd9cec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ca791fc471e34b9da2f9070fc1053c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23863bc37a8645029934b8c106622c51",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5ab5f08afa5841709aedb2f78a52a11c",
            "value": "â€‡20/20â€‡[00:52&lt;00:00,â€‡â€‡4.50s/it]"
          }
        },
        "d1d54ccd56494c4d831f71b416a1f880": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddffd834e09940a4bd3874c3f39b4e21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c233ad01604540a6c873f4a731982d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b44cb0f8e34446c8dde668a75d3d8ad",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_edaac6587b2d4bd5be52b89bb097f99f",
            "value": ""
          }
        },
        "e9a01115c75b499884f7e0ef32e9e599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb241365f604419af454c1c28de197a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cf586576ff44dba86ba2eb389593c61",
            "value": 1
          }
        },
        "edaac6587b2d4bd5be52b89bb097f99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef25efa751304e4699910f1fbc14345f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef63c3b2d51e452da03cdae5d9b034be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3665a86662746c4ac7cb0796604781d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
